{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only necessary if you're running Python 2.7 or lower\n",
    "from __future__ import print_function\n",
    "from __builtin__ import range\n",
    "\n",
    "# import matplotlib and define our alias\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot figures within the notebook rather than externally\n",
    "%matplotlib inline\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# scipy \n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This week, we'll be diving into some of the details involved with fitting models to data building on the concepts Prof. John Johnson taught last week using **linear regression**. Although linear regression appears simple at first glance, it actually has a surprisingly amount of depth. Most importantly, it provides an accessible way to get a handle on several big concepts in data analysis (and especially model fitting) and illustrate how to apply them in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we have a bunch of observations of [**Cepheid variable stars**](https://en.wikipedia.org/wiki/Classical_Cepheid_variable). Cepheids have a well-defined relationship between their **period** and **luminosity**, which allows astronomers to use them as [standard candles](https://en.wikipedia.org/wiki/Cosmic_distance_ladder#Standard_candles) when trying to calibrate distances. For each Cepheid we observe their **V-band [magnitude](https://en.wikipedia.org/wiki/Magnitude_(astronomy)** (a logarithmic measure of luminosity) $M_V$ as well as their period $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our **independent variable** (regressor, explanatory variable, etc.) to be $x \\equiv \\log_{10} \\left(\\frac{P}{10\\,\\textrm{days}} \\right)$ and our **dependent variable** (regressand, response variable, etc.) to be $y \\equiv M_v$. Our **model** for the data is then\n",
    "\n",
    "$$ y = \\alpha + \\beta x \\quad . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use [this](https://en.wikipedia.org/wiki/Classical_Cepheid_variable#Period-luminosity_relation) period-luminosity relationship\n",
    "\n",
    "$$ y = -2.43x - 4.05 $$\n",
    "\n",
    "to generate some mock data.\n",
    "\n",
    "Let's assume we have 10 stars **[uniformly distributed](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)** in the range $[-8, -4)$ (i.e. from -6 to -4, including -6 but excluding -4) and that we can measure $x$ and $y$ perfectly (we'll get back to errors later). The only uncertainty is some **intrinsic scatter** around the relation, which constitutes an error term $\\epsilon$ which we will take to be **[normally distributed](https://en.wikipedia.org/wiki/Normal_distribution)** with mean $\\mu=0$ and standard deviation $\\sigma=0.3$. So our final model looks like this:\n",
    "\n",
    "$$ x_i \\sim \\textrm{Unif}(-6, -4) $$\n",
    "$$ \\epsilon_i \\sim \\textrm{Normal}(\\mu=0, \\sigma=0.3) $$\n",
    "$$ y_i = -2.43x_i - 4.05 + \\epsilon_i $$\n",
    "\n",
    "where the index $i$ just is telling us we observed this quantity for object $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the period-luminosity relationship\n",
    "def plr(x):\n",
    "    return -2.43 * x - 4.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate random data points from the cepheid period-luminosity relationship\n",
    "\n",
    "n = 10  # number of objects\n",
    "sigma = 0.3  # intrinsic scatter\n",
    "\n",
    "x = np.random.uniform(-6, -4., n)  # absolute v-band magnitude\n",
    "e = np.random.normal(0.0, sigma, n)  # scatter\n",
    "y = plr(x) + e  # observed log(period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a few seconds to play around with the random number generation to get a sense for what these data look like.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting results\n",
    "plt.plot(x, y, 'o')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's assume we have no idea what the actual period-luminosity relationship actually is, and we would like to fit for our coefficients $\\alpha$ and $\\beta$. What determines a \"good\" fit? And how much \"better\" is one fit compared to another? Think about how we would quantify this and discuss your thinking with your classmates.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Bayes View (Linear Least Squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a more \"data mining\"-oriented perspective, the way to solve this problem is to define an appropriate **loss function** $L(\\boldsymbol{\\Theta} \\,|\\, \\mathbf{D})$ that we hope to minimize, where $\\boldsymbol{\\Theta}$ contains all the parameters of interest (here $\\alpha$ and $\\beta$) and $\\mathbf{D}$ contains all our data. Defining $\\mathbf{x} \\equiv \\lbrace x_1, \\dots, x_n \\rbrace$ and $\\mathbf{y} \\equiv \\lbrace y_1, \\dots, y_n \\rbrace$, let's write our loss function as the sum of the *squares* of the **residuals**\n",
    "\n",
    "$$ L(\\alpha, \\beta \\,|\\, \\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{n} \\left( \\Delta y_i \\right)^2 \\equiv \\sum_{i=1}^{n} \\left( y(x) - y_i \\right)^2 = \\sum_{i=1}^{n} \\left( \\alpha + \\beta x_i - y_i \\right)^2 \\quad . $$\n",
    "\n",
    "Let's define our linear relationship and this particular loss function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linear fit (takes input *vector* `theta`)\n",
    "def linear(theta):\n",
    "    return theta[0] + theta[1] * x\n",
    "\n",
    "# loss function\n",
    "def loss(theta):\n",
    "    return sum((linear(theta) - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing Our Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize our loss function, we need to find the [critical points](https://en.wikipedia.org/wiki/Critical_point). As a quick review, these are just all the points where\n",
    "\n",
    "$$ \\frac{\\partial f(\\boldsymbol{\\Theta})}{\\partial \\Theta_i} = 0 $$\n",
    "\n",
    "for all parameters $\\Theta_i$ of interest within $\\boldsymbol{\\Theta}$. In our case, we get:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\alpha} = \\sum_{i=1}^{n} 2(\\alpha + \\beta x_i - y_i) = 0 $$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta} = \\sum_{i=1}^{n} 2\\beta(\\alpha + \\beta x_i - y_i) = 0 $$\n",
    "\n",
    "This gives us two linear equations with two unknowns, which we can solve *exactly* using linear algebra to get an analytic best-fit solution $\\hat{\\alpha}$ and $\\hat{\\beta}$. You're welcome to try and solve this explicitly now; we'll come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll minimize our loss function using the `minimize` package contained within `scipy.optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import minimize\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the [documentation](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.optimize.minimize.html), see if you can figure out how to use `minimize` to get the best-fit parameters `theta` based on our loss function `loss`.** Spend some time digging around to see if you can understand both how the output is stored (as a [`dict`](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)) and what some of the terms mean. We'll get back to these quantities in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minimizing\n",
    "results = minimize(...)\n",
    "\n",
    "# best-fit solution\n",
    "theta_res = results[...]\n",
    "\n",
    "# print results\n",
    "print(results)\n",
    "print('Truth:', [-4.05, -2.43])\n",
    "print('Fit:', theta_res)\n",
    "\n",
    "# plot results\n",
    "plt.plot(x, y, 'o', label='observed')\n",
    "plt.plot(x, linear(theta_res), 'o', label='predicted')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back, the way we did things above seems very \"sloppy\". In particular, what's the motivation for picking our loss function? Why sum of squares instead of, say, a sum of cubes, $\\sum_i(\\Delta y_i)^3$, or a sum of absolute values, $\\sum_i |\\Delta y_i|$? The choice of loss function can dramatically affect our final results by changing how much information/weight each point contributes to the fit.\n",
    "\n",
    "From the Bayesian point of view, the above analysis has two critical flaws: \n",
    "1. the loss function is defined in an \"ad hoc\" way and\n",
    "2. our *prior beliefs* are not specified anywhere.\n",
    "\n",
    "Since the \"loss function\" is ultimately just an extension of our underlying model, choosing a loss function means we're actually implicitly choosing a model. Based on our knowledge/assumptions about the data and our model then, we should be able to *derive* our loss function directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, Bayes' Theorem can be written as\n",
    "\n",
    "$$ P(\\alpha, \\beta \\,|\\, \\mathbf{y}) = \\frac{P(\\mathbf{y} \\,|\\, \\alpha, \\beta) \\, P(\\alpha, \\beta)}{P(\\mathbf{y})} $$\n",
    "\n",
    "where $P(\\alpha, \\beta)$ is the **prior** on $\\alpha$ and $\\beta$, $P(\\mathbf{y} \\,|\\, \\alpha, \\beta)$ is the **likelihood** of $\\mathbf{y}$ given $\\alpha$ and $\\beta$, $P(\\mathbf{y})$ is the **evidence** for $\\mathbf{y}$ over all possible $\\alpha$ and $\\beta$, and $P(\\alpha, \\beta \\,|\\, \\mathbf{y})$ is the posterior (what we're usually interested in) for $\\alpha$ and $\\beta$ given $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our original model was\n",
    "\n",
    "$$ \\epsilon_i \\sim \\textrm{Normal}(\\mu=0, \\sigma=0.3) $$\n",
    "$$ y_i = -2.43x_i - 4.05 + \\epsilon_i $$\n",
    "\n",
    "For a given $\\alpha$ and $\\beta$, we then expect that our residuals $\\Delta y_i$ should follow the same distribution as our noise $\\epsilon_i$ (normally distributed with mean $\\mu=0$ and standard deviation $\\sigma=0.3$). The probability of observing residual $\\Delta y_i$ conditioned on our model parameters $\\alpha$ and $\\beta$ (i.e. the likelihood) is then\n",
    "\n",
    "$$ P(\\Delta y_i \\,|\\, \\alpha, \\beta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp \\left[ - \\frac{(\\Delta y_i)^2}{2\\sigma^2} \\right] \\quad .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normal distribution\n",
    "def normal(x, mu, sigma):\n",
    "    norm = 1. / (sigma * np.sqrt(2 * np.pi))\n",
    "    return norm * np.exp( - (mu - x)**2 / (2 * sigma**2))\n",
    "\n",
    "# plot locations on distribution\n",
    "xgrid = np.linspace(-1.5, 1.5, 100)\n",
    "ygrid = normal(xgrid, 0, sigma)\n",
    "resid = linear(theta_res) - y\n",
    "plt.plot(xgrid, ygrid)\n",
    "plt.plot(resid, normal(resid, 0, sigma), 'o')\n",
    "plt.xlabel('Jitter')\n",
    "plt.ylabel('PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming all our data points are independent, the probabilities all multiply, giving us\n",
    "\n",
    "$$ P(\\Delta \\mathbf{y} \\,|\\, \\alpha, \\beta) = \\prod_{i=1}^{n} P(\\Delta y_i \\,|\\, \\alpha, \\beta) = \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^n \\exp \\left[ - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (\\Delta y_i)^2 \\right] \\quad . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the likelihood and \"log-likelihood\" $\\ln P(\\Delta \\mathbf{y} \\,|\\, \\alpha, \\beta)$ for the best-fit parameters you derived above. Do those numbers look reasonable? Why or why not?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# likelihood/loglikelihood\n",
    "\n",
    "print('Like:', ...)\n",
    "print('Logl:', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The \"Meaning\" of Our Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last term inside the exponential should look extremely familiar -- it's our squared loss function from before!\n",
    "\n",
    "More explicitly,\n",
    "\n",
    "$$ L(\\alpha,\\beta \\,|\\, \\mathbf{x}, \\mathbf{y}) = -2\\sigma^2 \\left[ \\ln P(\\Delta \\mathbf{y} \\,|\\, \\alpha, \\beta) + n \\ln(\\sigma \\sqrt{2\\pi}) \\right] \\quad \\Rightarrow \\quad -A \\times L(\\alpha,\\beta \\,|\\, \\mathbf{x}, \\mathbf{y}) + B = \\ln P(\\Delta \\mathbf{y} \\,|\\, \\alpha, \\beta) $$\n",
    "\n",
    "where $A$ and $B$ are arbitrary constants. Since multiplicate/additive constants distribute across all terms within our loss function, they don't actually affect the minimum value in any way. **Minimizing a squared loss is thus equivalent to maximizing the likelihood under the assumption of uniform Gaussian noise**. So our choice of a squared loss above actually not only is a good choice, it is the **only choice that makes any sense given our underlying model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Aside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, I want to emphasize two things. First, **in a Bayesian paradigm this result and our interpretation of it is fundamentally dependent on our model.** In other words, if we assume, for instance, a different type of noise or that the errors are not all equal, then our likelihood (and the equivalent loss function) will change. So switching to different loss functions means we're switching to an entirely different model. This might be appropriate if we don't actually know what the appropriate model is (model selection is hard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, **there's no need to shackle ourselves to a strictly Bayesian paradigm**. Tons of real-world applications modify loss functions in ways that can be difficult to translate back into a full Bayesian model but are straightforward to interpret algorithmically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improper Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final addendum: although our results above worked with the likelihood, we actually need it to work with the posterior. This is not a big deal though: taking the log of Bayes theorem gives us\n",
    "\n",
    "$$ \\ln P(\\alpha, \\beta \\,|\\, \\mathbf{y}) = \\ln P(\\mathbf{y} \\,|\\, \\alpha, \\beta) + \\ln P(\\alpha, \\beta) - \\ln P(\\mathbf{y}) $$\n",
    "\n",
    "so if we just set $P(\\alpha, \\beta)$ to be a constant everywhere ($P(\\mathbf{y})$ is already a constant) our **maximum-likelihood** results will not change. \n",
    "\n",
    "**Take a second to think about what exactly we've done here, since this is a deep but subtle point. What does our prior actually do? Is it even a prior? Is this physically/philosophically possible? Discuss these issues with your neighbor and try to construct another example of a similar type of this type of \"improper\" prior.**\n",
    "\n",
    "**Extra Challenge: Can you construct the prior used here using limiting behavior from other, well-defined priors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship to $\\chi^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, Prof. Johnson mentioned this thing called **chi-square** ($\\chi^2$), which is defined as\n",
    "\n",
    "$$ \\chi^2(\\alpha, \\beta \\,|\\, \\mathbf{x}, \\mathbf{y}) \\equiv \\sum_{i=1}^{n} \\frac{\\left( \\Delta y_i \\right)^2}{\\sigma_i^2} = \\sum_{i=1}^{n} \\frac{\\left( y(x) - y_i \\right)^2}{\\sigma_i^2} = \\sigma^{-2} L(\\alpha, \\beta \\,|\\, \\mathbf{x}, \\mathbf{y}) \\quad . $$\n",
    "\n",
    "This can most straightforwardly be interpreted as an **error-weighted** metric for determining the goodness-of-fit, where the weights $w_i = \\sigma_i^{-2}$. These make a lot of sense given the $\\sigma^{-2}$ term that arises within our assumption of Normal errors. This perspective is also useful since these types of error-weighted metrics can be easily generalized to all sorts of problems.\n",
    "\n",
    "In our particular case where all the errors are the same ($\\sigma_1 = \\dots = \\sigma_n = \\sigma$; a phenomenon known as **homoscedasticity**), we see that all this does is add a multiplicative normalization to our loss function. As we shown above, this doesn't affect our **maximum-likelihood** (i.e. best-fit, minimum-loss, etc.) solution at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This definition of $\\chi^2$, however, can also be interpreted another way: instead of just adding up residuals, we want to first **normalize** each residual $\\Delta y_i$ by the **expected dispersion** ($\\sigma_i$). If we imagine that we have perfectly modeled the underlying relationship, the residuals should just be random noise. We can thus define $\\chi^2$ as:\n",
    "\n",
    "$$ \\chi^2 = \\left(\\frac{X_1 - \\mu_1}{\\sigma_1}\\right)^2 + \\dots + \\left(\\frac{X_n - \\mu_n}{\\sigma_n}\\right)^2 $$ \n",
    "$$ X_1, \\dots, X_n \\overset{i.i.d.}{\\sim} \\textrm{Normal}(\\mu_1,\\sigma_1), \\dots, \\textrm{Normal}(\\mu_n,\\sigma_n) $$\n",
    "\n",
    "There's a really neat property of normal random variables we can exploit here: all normal random variables can be written as shifted and scaled version of the **standard normal** as\n",
    "\n",
    "$$ X_i \\sim \\sigma_i Z_i + \\mu_i \\quad , \\quad Z_i \\sim \\textrm{Normal}(0, 1) \\quad . $$\n",
    "\n",
    "This enables us to rewrite our original expression as\n",
    "\n",
    "$$ \\chi^2 = \\left(\\frac{X_1 - \\mu_1}{\\sigma_1}\\right)^2 + \\dots + \\left(\\frac{X_n - \\mu_n}{\\sigma_n}\\right)^2 = Z_1^2 + \\dots + Z_n^2 $$\n",
    "\n",
    "where all quantities are defined as above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we expect $\\chi^2$ to have a particular *[distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution)* (specifically, a **probability density function or PDF**) based on the sum of squares of $n$ standard normal random variables. I can't emphasize enough how *Incredibly Important* this fact is. This singular result tells us two main things:\n",
    "1. We *expect* some amount of dispersion from our results. Too much dispersion means we're **under-fitting** (we need a better model), while too little dispersion means we're **over-fitting** (our model is \"too good\" and likely fitting noise).\n",
    "2. We actually have a theoretical distribution $P(\\chi^2 \\,|\\, \\textrm{dof})$ that tells us the probability of observing $\\chi^2$ to have a certain value for a given **degree of freedom** (dof).\n",
    "\n",
    "Let's simulate this explicitly below. **Using the provided code snippets, generate realizations of our $\\chi^2$ distributions for different numbers of data points (`npoints`) to see how the distribution changes as we add more data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npoints, nrepeats = 10, 10000  # number of points and repeat realizations\n",
    "\n",
    "# generate standard normal N(0,1) distributed random numbers with shape (npoints, nrepeats)\n",
    "# square all entries\n",
    "# sum up all results along 0th axis (sum over `npoints` axis; leaves `nrepeats` number of entries)\n",
    "chi2 = (np.random.normal(0., 1., size=(npoints, nrepeats))**2).sum(axis=0)\n",
    "\n",
    "# plot results\n",
    "plt.hist(chi2, bins=nrepeats/100, normed=True)\n",
    "plt.xlabel('$\\chi^2$')\n",
    "plt.ylabel('PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\chi^2$ distribution actually has a closed-form solution, which is accessible (along with many other distributions) via [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html). Let's use the analytic results stored there to check our numerical results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "xgrid = np.linspace(0., n*3., 1000)\n",
    "ygrid = stats.chi2.pdf(xgrid, n)\n",
    "chi2 = (np.random.normal(0., 1., size=(npoints, nrepeats))**2).sum(axis=0)\n",
    "\n",
    "# plot results\n",
    "plt.hist(chi2, bins=nrepeats/100, normed=True)\n",
    "plt.plot(xgrid, ygrid, color='red', lw=5, alpha=0.7)\n",
    "plt.xlabel('$\\chi^2$')\n",
    "plt.ylabel('PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding \"Effective\" Degrees of Freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result, however, where the $\\textrm{dof}=n$, only holds theoretically. In practice, the process of actually finding our best-fit solution *removes* degrees of freedom from our final result. For example, if we had two data points, our best-fit line would always fit them perfectly, leaving us with $\\chi^2 = 0$. This means we actually want to compare our result to $P(\\chi^2 \\,|\\, \\textrm{dof} = n - p)$ with an **effective dof** of $n-p$, where $n$ is the number of data points and $p$ is the number of **free parameters**. (You can prove this result, but I've left it out since it's a bit more involved and not really relevant for our purposes.)\n",
    "\n",
    "To illustrate this, in the cell below I've gone through and simulated our entire fit a bunch of times and have computed the $\\chi^2$ from each fit. The underlying $\\chi^2(n)$ and $\\chi^2(n-p)$ distributions are over-plotted for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minimizing\n",
    "npoints, nrepeats = 10, 10000  # number of points and repeat realizations\n",
    "chi2_arr = np.empty(nrepeats)\n",
    "for i in range(nrepeats):\n",
    "    xt = np.random.uniform(-6, -4., npoints)  # temp x values\n",
    "    et = np.random.normal(0.0, sigma, npoints)  # temp y truth values\n",
    "    yt = plr(xt) + et  # observed log(period)  # temp y obs values\n",
    "    rt = minimize(lambda theta: sum((theta[0] + theta[1] * xt - yt)**2) / sigma**2, [0., 2.])  # minimize loss\n",
    "    chi2_arr[i] = rt['fun']  # extract chi2 value\n",
    "\n",
    "# plot results\n",
    "plt.hist(chi2_arr, bins=nrepeats/100, normed=True)  # simulated values\n",
    "plt.plot(xgrid, stats.chi2.pdf(xgrid, n), color='red', lw=5, alpha=0.7, label='dof=n')  # chi2 with dof=n\n",
    "plt.plot(xgrid, stats.chi2.pdf(xgrid, n - 2), lw=5, alpha=0.7, label='dof=n-2')  # chi2 with dof=n-2\n",
    "plt.legend()\n",
    "plt.xlabel('$\\chi^2$')\n",
    "plt.ylabel('PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is extremely useful when trying to compare results from different models, which we'll take advantage of in just a second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `polyfit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cases above where we're dealing with least-squares minimization, `numpy` actually has a method [`polyfit`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) that computes the full analytic solution for any degree polynomial (which we'll hopefully come back to later). It also has a nice built-in class [`poly1d`](https://docs.scipy.org/doc/numpy/reference/routines.polynomials.poly1d.html) that automatically turns those solutions into functions!\n",
    "\n",
    "**Compare the solution from `polyfit` to the one you calculated earlier. How does it look? Try adding more polynomial terms to the fit, up to $b_0 + b_1 x + \\dots + b_{n-1}x^{n-1}$. Does it improve? How much did you *expect* it to improve, given that you're adding additional free parameters to the fit?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example code\n",
    "theta_fit = np.polyfit(x, y, 1)\n",
    "func = np.poly1d(theta_fit)\n",
    "func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compare our best-fit models to see if the data itself can reasonably discriminate against our different models (starting from a line with $p=2$ parameters and progressing to more complex polynomials). First, using our results from earlier, we know that we can evaluate the relative probabilities using our $P(\\chi^2 \\,|\\, n-p)$ distribution. One easy test is then just taking the ratio of these probabilities to get an **odds ratio** \n",
    "\n",
    "$$ R_{ij} = \\frac{P(\\chi^2 \\,|\\, n-p_i)}{P(\\chi^2 \\,|\\, n-p_j)} \\quad . $$ \n",
    "\n",
    "This tells us, in a relative sense, how much more likely one best-fit model with $p_i$ parameters is compared to another model with $p_j$ parameters. The \"rule of thumb\" for chi-square distributions is that the odds ratio should be around a factor of $e$ (i.e. the log-odds $\\ln R_{ij} \\sim 1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define our chi2 function (compact)\n",
    "def chi2(nparams):\n",
    "    return sum((y - np.poly1d(np.polyfit(x, y, nparams-1))(x))**2 / sigma**2)\n",
    "\n",
    "# plot results (probabilities)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)  # 1 row, 2 columns, subplot 1\n",
    "for i in range(2, n + 1):\n",
    "    plt.plot(i, stats.chi2.pdf(chi2(i), n-i), 'ko')\n",
    "    plt.xlabel('# of free parameters')\n",
    "    plt.ylabel('Probability')\n",
    "    \n",
    "# plot results (log odds ratio)\n",
    "plt.subplot(122)\n",
    "for i in range(2, n-1):\n",
    "    plt.plot(i+1, np.log(stats.chi2.pdf(chi2(i+1), n-i-1) / stats.chi2.pdf(chi2(i), n-i)), 'ko')\n",
    "plt.xticks(range(3, n+1), ['{0}/{1}'.format(j+1, j) for j in range(2, n-1)])\n",
    "plt.xlabel('# of free parameters')\n",
    "plt.ylabel('Log-Odds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's the end of this introductory notebook. In the next notebook, we'll look into how to derive **errors** on our best-fit models."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
