{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving Errors: An Example with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only necessary if you're running Python 2.7 or lower\n",
    "from __future__ import print_function\n",
    "from __builtin__ import range\n",
    "\n",
    "# import matplotlib and define our alias\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot figures within the notebook rather than externally\n",
    "%matplotlib inline\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# scipy \n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original statistical model looked like this:\n",
    "\n",
    "$$ x_i \\sim \\textrm{Unif}(-6, -4) $$\n",
    "$$ \\epsilon_i \\sim \\textrm{Normal}(\\mu=0, \\sigma=0.3) $$\n",
    "$$ y_i = -2.43x_i - 4.05 + \\epsilon_i $$\n",
    "\n",
    "where the index $i$ just is telling us we observed this quantity for object $i$. Here, we assume that our $x_i$'s are error-free (that we can essentially measure them perfectly), and treat our observations as having errors $\\epsilon_i$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the linear relationship between x and y\n",
    "def yfunc(x):\n",
    "    return -4.05 - 2.43 * x \n",
    "theta_true = [-4.05, -2.43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate random data points from the cepheid period-luminosity relationship\n",
    "n = 10  # number of objects\n",
    "sigma = 0.3  # intrinsic scatter\n",
    "\n",
    "x = np.random.uniform(-6, -4., n)  # x's\n",
    "y = yfunc(x)  # y's\n",
    "e = np.random.normal(0.0, sigma, n)  # errors\n",
    "yobs = y + e  # *observed* y's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting results\n",
    "plt.plot(x, y)  # linear relationship\n",
    "plt.errorbar(x, yobs, yerr=sigma, fmt='ko', ecolor='red')  # observed data\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum a Posteriori (MAP) Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the notation from the previous notebook, we can write Bayes' Theorem as\n",
    "\n",
    "$$ P(\\alpha, \\beta \\,|\\, \\mathbf{y}) = \\frac{P(\\mathbf{y} \\,|\\, \\alpha, \\beta) \\, P(\\alpha, \\beta)}{P(\\mathbf{y})} \\propto P(\\mathbf{y} \\,|\\, \\alpha, \\beta) \\, P(\\alpha, \\beta) $$\n",
    "\n",
    "where $P(\\alpha, \\beta)$ is the **prior** on $\\alpha$ and $\\beta$, $P(\\mathbf{y} \\,|\\, \\alpha, \\beta)$ is the **likelihood** of $\\mathbf{y}$ given $\\alpha$ and $\\beta$, $P(\\mathbf{y})$ is the **evidence** for $\\mathbf{y}$ over all possible $\\alpha$ and $\\beta$, and $P(\\alpha, \\beta \\,|\\, \\mathbf{y})$ is the posterior (what we're usually interested in) for $\\alpha$ and $\\beta$ given $\\mathbf{y}$. We will again ignore the evidence since it's just a normalization constant here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we showed that **least-squares** minimization, **$\\chi^2$** minimization, and **maximum-likelihood** analyses are all fundamentally equivalent in our problem and give us exactly the same solution. We also pointed out that this result extends to our posterior as well, *as long as our prior is uniform everywhere*. In other words, if $P(\\alpha, \\beta) = 1$, then maximum-likelihood analyses are also identical to **maximum a posteriori (MAP)** analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given $\\alpha$ and $\\beta$, we expect that our residuals $\\Delta y_i$ should follow the same distribution as our noise $\\epsilon_i$ (i.e. normally distributed with mean $\\mu=0$ and standard deviation $\\sigma=0.3$). The probability of observing residual $\\Delta y_i$ conditioned on our model parameters $\\alpha$ and $\\beta$ (i.e. the likelihood) is then\n",
    "\n",
    "$$ P(\\Delta y_i \\,|\\, \\alpha, \\beta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp \\left[ - \\frac{(\\Delta y_i)^2}{2\\sigma_i^2} \\right] \\quad .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming all our data points are independent and defining $\\Delta \\mathbf{y} \\equiv \\lbrace y_1, \\dots, y_n \\rbrace$, the probability of observing all of the residual values is\n",
    "\n",
    "$$ P(\\Delta \\mathbf{y} \\,|\\, \\alpha, \\beta) = \\prod_{i=1}^{n} P(\\Delta y_i \\,|\\, \\alpha, \\beta) \\propto \\exp \\left[ - \\frac{1}{2} \\sum_{i=1}^{n} \\frac{(\\Delta y_i)^2}{\\sigma_i^2} \\right] \\equiv \\exp \\left[ - \\frac{1}{2} \\times \\chi^2(\\alpha, \\beta) \\right] $$\n",
    "\n",
    "where we've again ignored scaling constants and have redefined the term inside the exponential as $\\chi^2$. So we can see that minimizing $\\chi^2$ is the same as maximizing our likelihood (which again is the same as maximizing our posterior since our prior is uniform everywhere)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear relationship\n",
    "def linear(theta):\n",
    "    return theta[0] + theta[1] * x\n",
    "\n",
    "# chi2\n",
    "def chi2(theta):\n",
    "    y = linear(theta)\n",
    "    resid = y - yobs\n",
    "    return sum((resid / sigma)**2)\n",
    "\n",
    "# posterior\n",
    "def posterior(theta):\n",
    "    return np.exp(-0.5 * chi2(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import minimize\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "results = minimize(fun=chi2, x0=[1., 1.])  # get best-fit solution (minimize chi2)\n",
    "theta_res = results['x']  # best-fit value\n",
    "\n",
    "print('Results:')\n",
    "print(results)\n",
    "print('Truth:', [-4.05, -2.43])\n",
    "print('Fit:', theta_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd now like to consider our *uncertainties* on our best-fit line. In other words, instead of asking \"What's the line that best fits our data?\", we instead want to ask \"Out of all the possible lines that *could* fit our data, what is the probability that a particular line is the One True Line?\". It turns out that this (seemingly) innocuous question actually requires quite a bit of work. Today we're going to go through three examples of deriving and visualizing errors:\n",
    "- a quick, approximate method (using the outputs from `minimize`),\n",
    "- a slow, exact method (a brute-force **grid search**),\n",
    "- and a hybrid, **Monte Carlo** method (importance sampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Inverse-Hessian Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the best-fit solution $\\hat{\\boldsymbol{\\Theta}}$, we can *approximate* the errors using the matrix composed of the second derivatives (known as the **Hessian**) $\\mathbf{H}$, where the $i$th row and $j$th column are defined as\n",
    "\n",
    "$$ H_{ij}(\\hat{\\boldsymbol{\\Theta}}) = \\frac{\\partial \\chi^2(\\hat{\\boldsymbol{\\Theta}})}{\\partial \\Theta_i \\partial \\Theta_j} \\quad . $$\n",
    "\n",
    "In our case, this is a 2x2 matrix with derivatives as a function of $\\alpha$ and $\\beta$. It turns out that *if we assume the errors are normally distributed in $p$ dimensions* (a strong assumption), the covariance matrix $\\mathbf{C}$ of this **multivariate normal** distribution is just\n",
    "\n",
    "$$ \\mathbf{C}(\\hat{\\alpha}, \\hat{\\beta}) \\approx \\left[\\mathbf{H}(\\hat{\\alpha}, \\hat{\\beta})\\right]^{-1} \\quad . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only reason I mention this at all is that this is actually one of the quantities (`'hess_inv'`) that `minimize` actually computes for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the `multivariate_normal` function in `np.random` to draw a bunch of samples from the approximate distribution of errors. What do the results look like? do they make sense?** \n",
    "\n",
    "**Bonus: Can you use `hist2d` instead of `plot` to better illustrate the change in number density?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# draw a set of samples from our approximate error distribution\n",
    "draws = np.random.multivariate_normal(...)\n",
    "\n",
    "# plot the results\n",
    "plt.plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corner Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really effective way to visualize the joint distribution between multiple parameters is to construct a **corner plot**. We're going to use the `corner` package by Dan Foreman-Mackey to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import corner\n",
    "from corner import corner\n",
    "\n",
    "# plot result in a corner plot\n",
    "fig = corner(draws, labels=['Intercept', 'Slope'], quantiles=[0.16, 0.5, 0.84], range=[[-8, -2], [-3.2, -1.7]],\n",
    "             show_titles=True, truths=theta_true, **{'plot_datapoints': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a few minutes to think about how this plot is set up and what each of the features are. Discuss your thoughts with your neighbors. Once you feel comfortable with the general setup, try playing around with the options (the [internal documentation](http://corner.readthedocs.io/en/latest/) will be helpful here).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the errors above might look nice, it's important to remember they're not quite right. We can, however, derive the exact errors using a brute-force approach known as a **grid search**. The basic idea is simple: we construct a grid in all the parameters of interest ($\\alpha$, $\\beta$), then simply compute the posterior at every grid point. In the end, we'll have sampled the posterior at every point, forming a *grid* of values across all possible parameters.\n",
    "\n",
    "First we need to define our grid in each dimension. **Define a grid in $\\alpha$ and $\\beta$ below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define 1-D grids\n",
    "alpha_grid = ...  # grid in alpha\n",
    "beta_grid = ...  # grid in beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define 2-D grid\n",
    "alphas, betas = np.meshgrid(alpha_grid, beta_grid, indexing='ij')  # mesh 1-D grids\n",
    "grid_points = np.c_[alphas.flatten(), betas.flatten()]  # combine into list of 2-D positions\n",
    "print(grid_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to derive the posterior at every chosen grid point. **Compute the posterior at each grid point and plot the result in `corner` using the `weights=` option.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute posterior at each grid point\n",
    "weights = ...\n",
    "\n",
    "# plot results\n",
    "fig = corner(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While a grid search is perfectly reasonable for simple models with a few free parameters, it quickly becomes infeasible for more complicated models. Why is this? How does the algorithm scale with dimensionality (i.e. the number of free parameters)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While computing our posterior estimates using a grid search is precise, it quickly becomes prohibitively slow in higher dimensions. It also isn't very efficient: we spend a lot of time computing posteriors for values of $(\\alpha, \\beta)$ that have $P(\\alpha, \\beta \\,|\\, \\mathbf{D}) \\approx 0$. A popular class of methods around this involve trying to use **randomness** to speed up the process by preferentially adding samples based on the amplitude of our posterior (i.e. higher probabilities). These are generally called **Monte Carlo** methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most straightforward Monte Carlo method is **importance sampling**. The basic idea there is that we draw samples based on an **importance distribution** $Q(\\alpha, \\beta)$ that we think is close to the *real* distribution (the posterior) $P(\\alpha, \\beta \\,|\\, \\mathbf{D})$. Each sample \n",
    "\n",
    "$$ (\\alpha_i, \\beta_i) \\sim Q(\\alpha, \\beta) $$ \n",
    "\n",
    "is then assigned a relative **importance weight**\n",
    "\n",
    "$$ w_i \\equiv \\frac{P(\\alpha_i, \\beta_i \\,|\\, \\mathbf{D})}{Q(\\alpha_i, \\beta_i)} \\quad . $$\n",
    "\n",
    "The collection of samples $\\lbrace (\\alpha_1, \\beta_1), \\dots, (\\alpha_N, \\beta_N) \\rbrace$ then can be used to get the full posterior distribution using these importance weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the reasoning behind defining these importance weights? Does is make sense that we can use them to recover the posterior? Take some time to discuss this with your neighbors and see if you can come up with an intuitive explanation for why this procedure works in general.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we actually have an excellent importance distribution available: the approximate error distribution provided by the Hessian. We also already computed a set of samples from the Hessian-inverse! So all that's left is to compute the importance weights for these samples.\n",
    "\n",
    "**Using the `posterior` function and the `pdf` function from `scipy.stats.multivariate_normal`, compute the importance weights for the set of samples `draws` computed above. Then plot the resulting posterior using `corner`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute importance weights\n",
    "imp = \n",
    "\n",
    "# plot results\n",
    "fig = corner(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's the end of this notebook!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
